{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pandas import Grouper\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from fbprophet import Prophet\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with some better data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In separate notebooks, we gathered the EMS, weather, and holiday data and did some initial cleaning steps. Below, we wrangle the data so it's ready for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_data = pd.read_csv('../data/ems_datetime_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns and ensure time is the index\n",
    "coerced_data = coerced_data.drop(['Unnamed: 0', 'INCIDENT_DATETIME'], axis=1).set_index('proper_time')\n",
    "coerced_data.index = pd.to_datetime(coerced_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our goals is to measure the frequency of calls over different time periods, so we need a way to tally calls when we call the \"resample\" method. Here we'll add a column where we assign a simple value of 1 to every call, and soon we'll use it to tally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_data['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INITIAL_CALL_TYPE</th>\n",
       "      <th>INITIAL_SEVERITY_LEVEL_CODE</th>\n",
       "      <th>FINAL_CALL_TYPE</th>\n",
       "      <th>FINAL_SEVERITY_LEVEL_CODE</th>\n",
       "      <th>VALID_DISPATCH_RSPNS_TIME_INDC</th>\n",
       "      <th>DISPATCH_RESPONSE_SECONDS_QY</th>\n",
       "      <th>VALID_INCIDENT_RSPNS_TIME_INDC</th>\n",
       "      <th>INCIDENT_RESPONSE_SECONDS_QY</th>\n",
       "      <th>HELD_INDICATOR</th>\n",
       "      <th>INCIDENT_DISPOSITION_CODE</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>POLICEPRECINCT</th>\n",
       "      <th>STANDBY_INDICATOR</th>\n",
       "      <th>Change_In_Severity</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proper_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01 00:00:04</th>\n",
       "      <td>RESPIR</td>\n",
       "      <td>4</td>\n",
       "      <td>RESPIR</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>101</td>\n",
       "      <td>Y</td>\n",
       "      <td>797.0</td>\n",
       "      <td>N</td>\n",
       "      <td>82.0</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10472.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 00:00:19</th>\n",
       "      <td>CARD</td>\n",
       "      <td>3</td>\n",
       "      <td>CARD</td>\n",
       "      <td>3</td>\n",
       "      <td>Y</td>\n",
       "      <td>59</td>\n",
       "      <td>Y</td>\n",
       "      <td>851.0</td>\n",
       "      <td>N</td>\n",
       "      <td>93.0</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 00:01:04</th>\n",
       "      <td>ARREST</td>\n",
       "      <td>1</td>\n",
       "      <td>ARREST</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>29</td>\n",
       "      <td>Y</td>\n",
       "      <td>429.0</td>\n",
       "      <td>N</td>\n",
       "      <td>83.0</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>11418.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 00:01:16</th>\n",
       "      <td>SICK</td>\n",
       "      <td>6</td>\n",
       "      <td>SICK</td>\n",
       "      <td>6</td>\n",
       "      <td>Y</td>\n",
       "      <td>56</td>\n",
       "      <td>Y</td>\n",
       "      <td>828.0</td>\n",
       "      <td>N</td>\n",
       "      <td>82.0</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10453.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01 00:01:26</th>\n",
       "      <td>INJURY</td>\n",
       "      <td>5</td>\n",
       "      <td>INJURY</td>\n",
       "      <td>5</td>\n",
       "      <td>Y</td>\n",
       "      <td>32</td>\n",
       "      <td>Y</td>\n",
       "      <td>856.0</td>\n",
       "      <td>N</td>\n",
       "      <td>82.0</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10457.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    INITIAL_CALL_TYPE  INITIAL_SEVERITY_LEVEL_CODE  \\\n",
       "proper_time                                                          \n",
       "2013-01-01 00:00:04            RESPIR                            4   \n",
       "2013-01-01 00:00:19              CARD                            3   \n",
       "2013-01-01 00:01:04            ARREST                            1   \n",
       "2013-01-01 00:01:16              SICK                            6   \n",
       "2013-01-01 00:01:26            INJURY                            5   \n",
       "\n",
       "                    FINAL_CALL_TYPE  FINAL_SEVERITY_LEVEL_CODE  \\\n",
       "proper_time                                                      \n",
       "2013-01-01 00:00:04          RESPIR                          4   \n",
       "2013-01-01 00:00:19            CARD                          3   \n",
       "2013-01-01 00:01:04          ARREST                          1   \n",
       "2013-01-01 00:01:16            SICK                          6   \n",
       "2013-01-01 00:01:26          INJURY                          5   \n",
       "\n",
       "                    VALID_DISPATCH_RSPNS_TIME_INDC  \\\n",
       "proper_time                                          \n",
       "2013-01-01 00:00:04                              Y   \n",
       "2013-01-01 00:00:19                              Y   \n",
       "2013-01-01 00:01:04                              Y   \n",
       "2013-01-01 00:01:16                              Y   \n",
       "2013-01-01 00:01:26                              Y   \n",
       "\n",
       "                     DISPATCH_RESPONSE_SECONDS_QY  \\\n",
       "proper_time                                         \n",
       "2013-01-01 00:00:04                           101   \n",
       "2013-01-01 00:00:19                            59   \n",
       "2013-01-01 00:01:04                            29   \n",
       "2013-01-01 00:01:16                            56   \n",
       "2013-01-01 00:01:26                            32   \n",
       "\n",
       "                    VALID_INCIDENT_RSPNS_TIME_INDC  \\\n",
       "proper_time                                          \n",
       "2013-01-01 00:00:04                              Y   \n",
       "2013-01-01 00:00:19                              Y   \n",
       "2013-01-01 00:01:04                              Y   \n",
       "2013-01-01 00:01:16                              Y   \n",
       "2013-01-01 00:01:26                              Y   \n",
       "\n",
       "                     INCIDENT_RESPONSE_SECONDS_QY HELD_INDICATOR  \\\n",
       "proper_time                                                        \n",
       "2013-01-01 00:00:04                         797.0              N   \n",
       "2013-01-01 00:00:19                         851.0              N   \n",
       "2013-01-01 00:01:04                         429.0              N   \n",
       "2013-01-01 00:01:16                         828.0              N   \n",
       "2013-01-01 00:01:26                         856.0              N   \n",
       "\n",
       "                     INCIDENT_DISPOSITION_CODE BOROUGH  ZIPCODE  \\\n",
       "proper_time                                                       \n",
       "2013-01-01 00:00:04                       82.0   BRONX  10472.0   \n",
       "2013-01-01 00:00:19                       93.0   BRONX  10454.0   \n",
       "2013-01-01 00:01:04                       83.0  QUEENS  11418.0   \n",
       "2013-01-01 00:01:16                       82.0   BRONX  10453.0   \n",
       "2013-01-01 00:01:26                       82.0   BRONX  10457.0   \n",
       "\n",
       "                     POLICEPRECINCT STANDBY_INDICATOR  Change_In_Severity  \\\n",
       "proper_time                                                                 \n",
       "2013-01-01 00:00:04            43.0                 N                   0   \n",
       "2013-01-01 00:00:19            40.0                 N                   0   \n",
       "2013-01-01 00:01:04           102.0                 N                   0   \n",
       "2013-01-01 00:01:16            46.0                 N                   0   \n",
       "2013-01-01 00:01:26            48.0                 N                   0   \n",
       "\n",
       "                     count  \n",
       "proper_time                 \n",
       "2013-01-01 00:00:04      1  \n",
       "2013-01-01 00:00:19      1  \n",
       "2013-01-01 00:01:04      1  \n",
       "2013-01-01 00:01:16      1  \n",
       "2013-01-01 00:01:26      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the EMS data looks good, let's add in some weather data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('../data/weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.set_index('Date', inplace=True)\n",
    "weather_data.index = pd.to_datetime(weather_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max Temp</th>\n",
       "      <th>Min Temp</th>\n",
       "      <th>Avg Temp</th>\n",
       "      <th>Precipitation Water Equiv</th>\n",
       "      <th>Snowfall</th>\n",
       "      <th>Snow/Ice Depth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>40</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>33</td>\n",
       "      <td>22.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>32</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>37</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>42</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Max Temp  Min Temp  Avg Temp  Precipitation Water Equiv  Snowfall  \\\n",
       "Date                                                                            \n",
       "2013-01-01        40      26.0      33.0                        0.0       0.0   \n",
       "2013-01-02        33      22.0      27.5                        0.0       0.0   \n",
       "2013-01-03        32      24.0      28.0                        0.0       0.0   \n",
       "2013-01-04        37      30.0      33.5                        0.0       0.0   \n",
       "2013-01-05        42      32.0      37.0                        0.0       0.0   \n",
       "\n",
       "            Snow/Ice Depth  \n",
       "Date                        \n",
       "2013-01-01             0.0  \n",
       "2013-01-02             0.0  \n",
       "2013-01-03             0.0  \n",
       "2013-01-04             0.0  \n",
       "2013-01-05             0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add in some holiday data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = pd.read_csv('../data/holiday_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data.set_index('Date', inplace=True)\n",
    "holiday_data.index = pd.to_datetime(holiday_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Holiday\n",
       "Date               \n",
       "2013-01-01     True\n",
       "2013-01-02    False\n",
       "2013-01-03    False\n",
       "2013-01-04    False\n",
       "2013-01-05    False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample all data so we see metrics grouped by week. For EMS data, we want the average response time per week and the total number of calls per week. For the weather data, we want the average weekly temperature, as well as the total rainfall and snowfall per week. Lastly, for the holiday data, we want the total number of holidays per week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume = coerced_data['count'].resample('W').sum()\n",
    "daily_call_volume = coerced_data['count'].resample('D').sum()\n",
    "weekly_average_response_time = coerced_data['INCIDENT_RESPONSE_SECONDS_QY'].resample('W').mean()\n",
    "daily_average_response_time = coerced_data['INCIDENT_RESPONSE_SECONDS_QY'].resample('D').mean()\n",
    "\n",
    "#modify our avearge response time data to measure time in minutes instead of seconds\n",
    "weekly_average_response_time = weekly_average_response_time / 60\n",
    "daily_average_response_time = daily_average_response_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_temperature_data = pd.DataFrame(weather_data['Avg Temp'].resample('W').mean())\n",
    "weekly_sum_precipitation = pd.DataFrame(weather_data['Precipitation Water Equiv'].resample('W').sum())\n",
    "weekly_sum_snowfall = pd.DataFrame(weather_data['Snowfall'].resample('W').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sum_holidays = holiday_data.resample('W').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame to merge average response time data with holiday and weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_response_time_df = pd.DataFrame(data=weekly_average_response_time, index=weekly_average_response_time.index)\n",
    "weekly_average_response_time_df = pd.merge(weekly_average_response_time_df, weekly_average_temperature_data, left_index=True, right_index=True)\n",
    "weekly_average_response_time_df = pd.merge(weekly_average_response_time_df, weekly_sum_precipitation, left_index=True, right_index=True)\n",
    "weekly_average_response_time_df = pd.merge(weekly_average_response_time_df, weekly_sum_snowfall, left_index=True, right_index=True)\n",
    "weekly_average_response_time_df = pd.merge(weekly_average_response_time_df, weekly_sum_holidays, left_index=True, right_index=True)\n",
    "\n",
    "#rename the columns so they are easier to understand and reference\n",
    "weekly_average_response_time_df.columns = ['avg_response_time_min', 'avg_temp', 'total_precip', 'total_snowfall', 'total_holidays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_response_time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame to merge average call volume data with holiday and weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume_df = pd.DataFrame(data=weekly_call_volume, index=weekly_call_volume.index)\n",
    "weekly_call_volume_df = pd.merge(weekly_call_volume_df, weekly_average_temperature_data, left_index=True, right_index=True)\n",
    "weekly_call_volume_df = pd.merge(weekly_call_volume_df, weekly_sum_precipitation, left_index=True, right_index=True)\n",
    "weekly_call_volume_df = pd.merge(weekly_call_volume_df, weekly_sum_snowfall, left_index=True, right_index=True)\n",
    "weekly_call_volume_df = pd.merge(weekly_call_volume_df, weekly_sum_holidays, left_index=True, right_index=True)\n",
    "\n",
    "#rename the columns so they are easier to understand and reference\n",
    "weekly_call_volume_df.columns = ['sum of weekly calls', 'avg_temp', 'total_precip', 'total_snowfall', 'total_holidays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Stationarity Check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to see if there were any noticeable trends, so we explored the data through visualizations. Additionally, stationarity is a key assumption for time series models, so we used the Dickey-Fuller test statistic to test for stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to perform Dickey-Fuller test and print key statistics\n",
    "def dickey_fuller(ser):\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(ser.values)\n",
    "\n",
    "    # Extract and display test results in a user friendly manner\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for weekly average temperature \n",
    "plt.plot(weekly_average_response_time_df['avg_temp'])\n",
    "plt.show()\n",
    "dickey_fuller(weekly_average_response_time_df['avg_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for total weekly precipitation  \n",
    "plt.plot(weekly_average_response_time_df['total_precip'])\n",
    "plt.show()\n",
    "dickey_fuller(weekly_average_response_time_df['total_precip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for total weekly snowfall  \n",
    "plt.plot(weekly_average_response_time_df['total_snowfall'])\n",
    "plt.show()\n",
    "dickey_fuller(weekly_average_response_time_df['total_snowfall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for total number of holidays in a week \n",
    "plt.plot(weekly_average_response_time_df['total_holidays'])\n",
    "plt.show()\n",
    "dickey_fuller(weekly_average_response_time_df['total_holidays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for total number of calls in a week \n",
    "plt.plot(weekly_call_volume)\n",
    "plt.show()\n",
    "dickey_fuller(weekly_call_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA and Dickey-Fuller for weekly average response time \n",
    "plt.plot(weekly_average_response_time)\n",
    "plt.show()\n",
    "dickey_fuller(weekly_average_response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, we get a decent P-val for our stationarity check on the total number of calls in a week and the weekly average response time, but we know we can do better. There must be seasonality to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal decomposition for weekly average response time \n",
    "decomposition = seasonal_decompose(weekly_average_response_time, freq=52)  \n",
    "fig = plt.figure()  \n",
    "fig = decomposition.plot()  \n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal decomposition for total number of calls in a week \n",
    "decomposition = seasonal_decompose(weekly_average_response_time, freq=52)  \n",
    "fig = plt.figure()  \n",
    "fig = decomposition.plot()  \n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for average response time, found seasonal first difference to produce best results\n",
    "response_time_first_diff = weekly_average_response_time - weekly_average_response_time.shift(1)\n",
    "response_time_seasonal_first_difference = (response_time_first_diff - response_time_first_diff(52)).dropna()\n",
    "dickey_fuller(response_time_seasonal_first_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF and PCF for average response time \n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_acf(response_time_seasonal_first_difference, lags = 10);\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(response_time_seasonal_first_difference, lags = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for call volume, found seasonal first difference to produce best results\n",
    "call_volume_first_diff = weekly_call_volume - weekly_call_volume.shift(1)\n",
    "call_volume_seasonal_first_difference = (call_volume_first_diff - call_volume_first_diff(52)).dropna()\n",
    "dickey_fuller(call_volume_seasonal_first_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF and PCF for call volume\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_acf(call_volume_seasonal_first_difference, lags = 10);\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(call_volume_seasonal_first_difference, lags = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Model Average Response Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for ideal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, d, q))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a grid with pdq and seasonal pdq parameters calculated above and get the best AIC value\n",
    "ans = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(weekly_average_response_time,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}52 : AIC Calculated ={}'.format(comb, combs, output.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters with minimal AIC value.\n",
    "\n",
    "ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'aic'])\n",
    "ans_df.loc[ans_df['aic'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to split our data into training and test sets with an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endogenous_train = weekly_average_response_time[:-52]\n",
    "exogenous_train = weekly_average_response_time_df.drop(['avg_response_time_min'], axis=1)[:-52]\n",
    "endogenous_test = weekly_average_response_time[-52:]\n",
    "exogenous_test = weekly_average_response_time_df.drop(['avg_response_time_min'], axis=1)[-52:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit training data on SARIMAX model with optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(endogenous_train,\n",
    "                                exog = exogenous_train,\n",
    "                                order=(1, 0, 1),\n",
    "                                seasonal_order=(0, 1, 1, 52),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "output = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot_diagnostics(figsize=(15, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = output.get_prediction(start = endogenous_test.index[0],\n",
    "                                    end = endogenous_test.index[-1],\n",
    "                                    exog = exogenous_test,\n",
    "                                    dynamic = False)\n",
    "\n",
    "pred_conf = prediction.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our predictions look like against actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the static forecast with confidence intervals.\n",
    "\n",
    "ax = weekly_average_response_time[-100:].plot(label='observed', figsize=(18, 10))\n",
    "prediction.predicted_mean.plot(label='Static Forecast', ax=ax)\n",
    "\n",
    "# ax.fill_between(pred_conf.index,\n",
    "#                 pred_conf.iloc[:, 0],\n",
    "#                 pred_conf.iloc[:, 1], color='g', alpha=.3)\n",
    "\n",
    "# ax.fill_betweenx(ax.get_ylim(), \n",
    "#                  weekly_average_response_time[-50:-49].index[0], \n",
    "#                  '2017-12-31', \n",
    "#                  alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Avg Weekly Call Time')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some error metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((prediction.predicted_mean - endogenous_test)**2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "print('The MSE for this model is {}'.format(mse))\n",
    "print('The RMSE for this model is {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore the data more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_response_time_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_response_time_df.nlargest(10, 'avg_response_time_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_response_time_df.nsmallest(10, 'avg_response_time_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume_df.nlargest(10, 'sum of weekly calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_call_volume_df.nsmallest(10, 'sum of weekly calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Bayes]",
   "language": "python",
   "name": "conda-env-Bayes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
